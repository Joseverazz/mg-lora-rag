{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc81df1",
   "metadata": {},
   "source": [
    "# PRÁCTICA 2A: Fine-tuning Eficiente con LoRA\n",
    "\n",
    "**Objetivo**: Aprender técnicas de Parameter-Efficient Fine-Tuning (PEFT) con LoRA.\n",
    "\n",
    "En este notebook exploraremos:\n",
    "- **LoRA (Low-Rank Adaptation)**: Técnica de fine-tuning eficiente\n",
    "- **PEFT**: Parameter-Efficient Fine-Tuning concepts\n",
    "- **Comparación**: Entrenar solo ~1% de parámetros vs entrenar todo\n",
    "\n",
    "Métricas a observar:\n",
    "- Número de parámetros entrenables con LoRA\n",
    "- Tiempo de entrenamiento\n",
    "- Uso de memoria\n",
    "- Accuracy final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6ed931",
   "metadata": {},
   "source": [
    "## 1. Setup y Preparación del Dataset (5 min)\n",
    "\n",
    "**Requisitos previos:**\n",
    "- Python 3.8+\n",
    "- Paquetes: `transformers`, `datasets`, `peft`, `evaluate`, `torch`\n",
    "- Opcional: GPU con CUDA para acelerar (funciona también en CPU)\n",
    "\n",
    "**Nota**: Este notebook usa un subset pequeño del dataset (1000 ejemplos) para demostración rápida.\n",
    "Para resultados de producción, usa el dataset completo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0fa2ef",
   "metadata": {},
   "source": [
    "### Nota Importante sobre GPUs Múltiples\n",
    "\n",
    "Si tienes múltiples GPUs y experimentas errores NCCL, ejecuta esto **ANTES** de abrir el notebook:\n",
    "```bash\n",
    "export CUDA_VISIBLE_DEVICES=0\n",
    "```\n",
    "\n",
    "O reinicia el kernel después de ejecutar la primera celda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1643ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fdda73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# IMPORTANTE: Configurar para usar solo 1 GPU ANTES de importar transformers\n",
    "# Esto previene errores NCCL en sistemas con múltiples GPUs\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Para debugging si es necesario\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verificar GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memoria disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Número de GPUs visibles: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c4add",
   "metadata": {},
   "source": [
    "### 1.1 Cargar y Preparar el Dataset\n",
    "\n",
    "Usaremos el dataset **IMDB** para clasificación de sentimiento (positivo/negativo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2417b6cd",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8de1652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\universidad\\1\\universidad\\tfg\\tfg\\.venv\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-22.0.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\universidad\\1\\universidad\\tfg\\tfg\\.venv\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.6.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Using cached multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Collecting fsspec<=2025.10.0,>=2023.1.0 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Downloading huggingface_hub-1.1.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in c:\\universidad\\1\\universidad\\tfg\\tfg\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached pyyaml-6.0.3-cp312-cp312-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.13.2-cp312-cp312-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets)\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting certifi (from httpx<1.0.0->datasets)\n",
      "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1.0.0->datasets)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.8.0-cp312-cp312-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.7.0-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.4.1-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.22.0-cp312-cp312-win_amd64.whl.metadata (77 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.32.2->datasets)\n",
      "  Using cached charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl.metadata (38 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: colorama in c:\\universidad\\1\\universidad\\tfg\\tfg\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1.0.0->datasets)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\universidad\\1\\universidad\\tfg\\tfg\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\universidad\\1\\universidad\\tfg\\tfg\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\universidad\\1\\universidad\\tfg\\tfg\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\universidad\\1\\universidad\\tfg\\tfg\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-1.1.2-py3-none-any.whl (514 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 2.6/2.9 MB 13.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 12.1 MB/s  0:00:00\n",
      "Using cached multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "Using cached aiohttp-3.13.2-cp312-cp312-win_amd64.whl (453 kB)\n",
      "Using cached multidict-6.7.0-cp312-cp312-win_amd64.whl (46 kB)\n",
      "Using cached yarl-1.22.0-cp312-cp312-win_amd64.whl (87 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached frozenlist-1.8.0-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached propcache-0.4.1-cp312-cp312-win_amd64.whl (41 kB)\n",
      "Using cached pyarrow-22.0.0-cp312-cp312-win_amd64.whl (28.0 MB)\n",
      "Using cached pyyaml-6.0.3-cp312-cp312-win_amd64.whl (154 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl (107 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Using cached click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached xxhash-3.6.0-cp312-cp312-win_amd64.whl (31 kB)\n",
      "Installing collected packages: xxhash, urllib3, typing-extensions, tqdm, sniffio, shellingham, pyyaml, pyarrow, propcache, multidict, idna, hf-xet, h11, fsspec, frozenlist, filelock, dill, click, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, typer-slim, requests, multiprocess, httpcore, anyio, aiosignal, httpx, aiohttp, huggingface-hub, datasets\n",
      "\n",
      "   - --------------------------------------  1/33 [urllib3]\n",
      "   - --------------------------------------  1/33 [urllib3]\n",
      "   - --------------------------------------  1/33 [urllib3]\n",
      "   - --------------------------------------  1/33 [urllib3]\n",
      "   - --------------------------------------  1/33 [urllib3]\n",
      "   - --------------------------------------  1/33 [urllib3]\n",
      "   - --------------------------------------  1/33 [urllib3]\n",
      "   - --------------------------------------  1/33 [urllib3]\n",
      "   - --------------------------------------  1/33 [urllib3]\n",
      "   -- -------------------------------------  2/33 [typing-extensions]\n",
      "   --- ------------------------------------  3/33 [tqdm]\n",
      "   --- ------------------------------------  3/33 [tqdm]\n",
      "   --- ------------------------------------  3/33 [tqdm]\n",
      "   --- ------------------------------------  3/33 [tqdm]\n",
      "   --- ------------------------------------  3/33 [tqdm]\n",
      "   --- ------------------------------------  3/33 [tqdm]\n",
      "   --- ------------------------------------  3/33 [tqdm]\n",
      "   ---- -----------------------------------  4/33 [sniffio]\n",
      "   ------ ---------------------------------  5/33 [shellingham]\n",
      "   ------ ---------------------------------  5/33 [shellingham]\n",
      "   ------- --------------------------------  6/33 [pyyaml]\n",
      "   ------- --------------------------------  6/33 [pyyaml]\n",
      "   ------- --------------------------------  6/33 [pyyaml]\n",
      "   ------- --------------------------------  6/33 [pyyaml]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   -------- -------------------------------  7/33 [pyarrow]\n",
      "   --------- ------------------------------  8/33 [propcache]\n",
      "   ------------ --------------------------- 10/33 [idna]\n",
      "   ------------ --------------------------- 10/33 [idna]\n",
      "   ------------ --------------------------- 10/33 [idna]\n",
      "   -------------- ------------------------- 12/33 [h11]\n",
      "   -------------- ------------------------- 12/33 [h11]\n",
      "   -------------- ------------------------- 12/33 [h11]\n",
      "   -------------- ------------------------- 12/33 [h11]\n",
      "   --------------- ------------------------ 13/33 [fsspec]\n",
      "   --------------- ------------------------ 13/33 [fsspec]\n",
      "   --------------- ------------------------ 13/33 [fsspec]\n",
      "   --------------- ------------------------ 13/33 [fsspec]\n",
      "   --------------- ------------------------ 13/33 [fsspec]\n",
      "   --------------- ------------------------ 13/33 [fsspec]\n",
      "   --------------- ------------------------ 13/33 [fsspec]\n",
      "   --------------- ------------------------ 13/33 [fsspec]\n",
      "   --------------- ------------------------ 13/33 [fsspec]\n",
      "   --------------- ------------------------ 13/33 [fsspec]\n",
      "   --------------- ------------------------ 13/33 [fsspec]\n",
      "   --------------- ------------------------ 13/33 [fsspec]\n",
      "   --------------- ------------------------ 13/33 [fsspec]\n",
      "   ---------------- ----------------------- 14/33 [frozenlist]\n",
      "   ------------------ --------------------- 15/33 [filelock]\n",
      "   ------------------- -------------------- 16/33 [dill]\n",
      "   ------------------- -------------------- 16/33 [dill]\n",
      "   ------------------- -------------------- 16/33 [dill]\n",
      "   ------------------- -------------------- 16/33 [dill]\n",
      "   ------------------- -------------------- 16/33 [dill]\n",
      "   ------------------- -------------------- 16/33 [dill]\n",
      "   ------------------- -------------------- 16/33 [dill]\n",
      "   ------------------- -------------------- 16/33 [dill]\n",
      "   ------------------- -------------------- 16/33 [dill]\n",
      "   ------------------- -------------------- 16/33 [dill]\n",
      "   ------------------- -------------------- 16/33 [dill]\n",
      "   ------------------- -------------------- 16/33 [dill]\n",
      "   -------------------- ------------------- 17/33 [click]\n",
      "   -------------------- ------------------- 17/33 [click]\n",
      "   -------------------- ------------------- 17/33 [click]\n",
      "   -------------------- ------------------- 17/33 [click]\n",
      "   --------------------- ------------------ 18/33 [charset_normalizer]\n",
      "   --------------------- ------------------ 18/33 [charset_normalizer]\n",
      "   --------------------- ------------------ 18/33 [charset_normalizer]\n",
      "   ----------------------- ---------------- 19/33 [certifi]\n",
      "   ------------------------ --------------- 20/33 [attrs]\n",
      "   ------------------------ --------------- 20/33 [attrs]\n",
      "   ------------------------ --------------- 20/33 [attrs]\n",
      "   ------------------------ --------------- 20/33 [attrs]\n",
      "   ------------------------- -------------- 21/33 [aiohappyeyeballs]\n",
      "   -------------------------- ------------- 22/33 [yarl]\n",
      "   -------------------------- ------------- 22/33 [yarl]\n",
      "   -------------------------- ------------- 22/33 [yarl]\n",
      "   --------------------------- ------------ 23/33 [typer-slim]\n",
      "   --------------------------- ------------ 23/33 [typer-slim]\n",
      "   --------------------------- ------------ 23/33 [typer-slim]\n",
      "   --------------------------- ------------ 23/33 [typer-slim]\n",
      "   ----------------------------- ---------- 24/33 [requests]\n",
      "   ----------------------------- ---------- 24/33 [requests]\n",
      "   ----------------------------- ---------- 24/33 [requests]\n",
      "   ----------------------------- ---------- 24/33 [requests]\n",
      "   ------------------------------ --------- 25/33 [multiprocess]\n",
      "   ------------------------------ --------- 25/33 [multiprocess]\n",
      "   ------------------------------ --------- 25/33 [multiprocess]\n",
      "   ------------------------------ --------- 25/33 [multiprocess]\n",
      "   ------------------------------ --------- 25/33 [multiprocess]\n",
      "   ------------------------------ --------- 25/33 [multiprocess]\n",
      "   ------------------------------ --------- 25/33 [multiprocess]\n",
      "   ------------------------------ --------- 25/33 [multiprocess]\n",
      "   ------------------------------ --------- 25/33 [multiprocess]\n",
      "   ------------------------------ --------- 25/33 [multiprocess]\n",
      "   ------------------------------- -------- 26/33 [httpcore]\n",
      "   ------------------------------- -------- 26/33 [httpcore]\n",
      "   ------------------------------- -------- 26/33 [httpcore]\n",
      "   ------------------------------- -------- 26/33 [httpcore]\n",
      "   ------------------------------- -------- 26/33 [httpcore]\n",
      "   ------------------------------- -------- 26/33 [httpcore]\n",
      "   ------------------------------- -------- 26/33 [httpcore]\n",
      "   ------------------------------- -------- 26/33 [httpcore]\n",
      "   -------------------------------- ------- 27/33 [anyio]\n",
      "   -------------------------------- ------- 27/33 [anyio]\n",
      "   -------------------------------- ------- 27/33 [anyio]\n",
      "   -------------------------------- ------- 27/33 [anyio]\n",
      "   -------------------------------- ------- 27/33 [anyio]\n",
      "   -------------------------------- ------- 27/33 [anyio]\n",
      "   -------------------------------- ------- 27/33 [anyio]\n",
      "   -------------------------------- ------- 27/33 [anyio]\n",
      "   -------------------------------- ------- 27/33 [anyio]\n",
      "   ----------------------------------- ---- 29/33 [httpx]\n",
      "   ----------------------------------- ---- 29/33 [httpx]\n",
      "   ----------------------------------- ---- 29/33 [httpx]\n",
      "   ----------------------------------- ---- 29/33 [httpx]\n",
      "   ----------------------------------- ---- 29/33 [httpx]\n",
      "   ----------------------------------- ---- 29/33 [httpx]\n",
      "   ----------------------------------- ---- 29/33 [httpx]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------ --- 30/33 [aiohttp]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   ------------------------------------- -- 31/33 [huggingface-hub]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   -------------------------------------- - 32/33 [datasets]\n",
      "   ---------------------------------------- 33/33 [datasets]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.11.0 attrs-25.4.0 certifi-2025.10.5 charset_normalizer-3.4.4 click-8.3.0 datasets-4.4.1 dill-0.4.0 filelock-3.20.0 frozenlist-1.8.0 fsspec-2025.10.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-1.1.2 idna-3.11 multidict-6.7.0 multiprocess-0.70.18 propcache-0.4.1 pyarrow-22.0.0 pyyaml-6.0.3 requests-2.32.5 shellingham-1.5.4 sniffio-1.3.1 tqdm-4.67.1 typer-slim-0.20.0 typing-extensions-4.15.0 urllib3-2.5.0 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc137f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Universidad\\1\\Universidad\\TFG\\TFG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset IMDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Universidad\\1\\Universidad\\TFG\\TFG\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:121: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pablo\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 224165.35 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 263660.71 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 273662.81 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estructura del dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "\n",
      "Ejemplo de muestra:\n",
      "Texto: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ev...\n",
      "Label: 0 (0=negativo, 1=positivo)\n",
      "\n",
      "Estadísticas:\n",
      "Train: 25000 ejemplos\n",
      "Test: 25000 ejemplos\n"
     ]
    }
   ],
   "source": [
    "# Cargar dataset IMDB\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Cargando dataset IMDB...\")\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "print(f\"\\nEstructura del dataset:\")\n",
    "print(dataset)\n",
    "\n",
    "print(f\"\\nEjemplo de muestra:\")\n",
    "print(f\"Texto: {dataset['train'][0]['text'][:200]}...\")\n",
    "print(f\"Label: {dataset['train'][0]['label']} (0=negativo, 1=positivo)\")\n",
    "\n",
    "# Estadísticas\n",
    "print(f\"\\nEstadísticas:\")\n",
    "print(f\"Train: {len(dataset['train'])} ejemplos\")\n",
    "print(f\"Test: {len(dataset['test'])} ejemplos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para acelerar el entrenamiento, usaremos un subset pequeño\n",
    "# Suficiente para demostrar el concepto y completar en ~5-10 minutos\n",
    "\n",
    "TRAIN_SIZE = 1000  # Número de ejemplos para entrenamiento (reducido para rapidez)\n",
    "TEST_SIZE = 300    # Número de ejemplos para evaluación\n",
    "\n",
    "# Crear subsets con shuffle para diversidad\n",
    "train_dataset = dataset['train'].shuffle(seed=42).select(range(TRAIN_SIZE))\n",
    "test_dataset = dataset['test'].shuffle(seed=42).select(range(TEST_SIZE))\n",
    "\n",
    "print(f\"Usando subset reducido:\")\n",
    "print(f\"Train: {len(train_dataset)} ejemplos\")\n",
    "print(f\"Test: {len(test_dataset)} ejemplos\")\n",
    "print(f\"\\nNota: Para producción usarías el dataset completo (25K ejemplos)\")\n",
    "print(f\"      Aquí priorizamos rapidez de demostración\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f4c8c",
   "metadata": {},
   "source": [
    "### 1.2 Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32af641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar tokenizer\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Función de tokenización\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokeniza los textos del dataset.\n",
    "    \n",
    "    Parámetros importantes:\n",
    "    - padding=\"max_length\": Rellena todas las secuencias a la misma longitud (max_length)\n",
    "                            Alternativas: \"longest\" (rellena al más largo del batch)\n",
    "    - truncation=True: Corta secuencias más largas que max_length\n",
    "    - max_length=512: Longitud máxima de la secuencia en tokens\n",
    "                      DistilBERT admite hasta 512 tokens\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],           # Textos a tokenizar\n",
    "        padding=\"max_length\",       # Rellenar todas las secuencias a max_length\n",
    "        truncation=True,            # Cortar si excede max_length\n",
    "        max_length=512              # Longitud máxima (límite del modelo)\n",
    "    )\n",
    "\n",
    "# Tokenizar datasets\n",
    "print(\"Tokenizando datasets...\")\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(\"\\nDatasets tokenizados:\")\n",
    "print(f\"Train: {tokenized_train}\")\n",
    "print(f\"Test: {tokenized_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d1b44e",
   "metadata": {},
   "source": [
    "# DataCollatorWithPadding\n",
    "\n",
    "DataCollatorWithPadding es un componente de Hugging Face Transformers que prepara los batches de datos durante el entrenamiento. Veamos qué hace y sus alternativas:\n",
    "\n",
    "¿Qué hace DataCollatorWithPadding?\n",
    "Función principal: Aplica padding dinámico a las secuencias dentro de cada batch para que todas tengan la misma longitud.\n",
    "\n",
    "Cómo funciona:\n",
    "\n",
    "Recibe un batch de ejemplos tokenizados\n",
    "Encuentra la secuencia más larga del batch\n",
    "Rellena (pad) todas las demás secuencias a esa longitud\n",
    "Crea las máscaras de atención automáticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985eeed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator para padding dinámico\n",
    "# Alternativa a padding=\"max_length\": permite diferentes longitudes por batch\n",
    "# Más eficiente en memoria cuando las secuencias tienen longitudes variadas\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,     # Tokenizer para aplicar padding\n",
    "    padding=True,            # Aplicar padding dinámico\n",
    "    max_length=None,         # Sin límite adicional (usa el del tokenizer)\n",
    "    pad_to_multiple_of=None  # Sin redondeo de longitud\n",
    ")\n",
    "\n",
    "# Métrica de evaluación\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calcula métricas de evaluación durante el entrenamiento.\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: Tuple con (predictions, labels)\n",
    "                   predictions: logits del modelo (shape: [batch_size, num_labels])\n",
    "                   labels: etiquetas verdaderas (shape: [batch_size])\n",
    "    \n",
    "    Returns:\n",
    "        Dict con métricas calculadas\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # Convertir logits a clases predichas (argmax sobre dimensión de clases)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # Calcular accuracy comparando predicciones con labels verdaderos\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8115c38",
   "metadata": {},
   "source": [
    "## 2. Full Fine-tuning: Entrenar Todos los Parámetros\n",
    "\n",
    "**Primero entrenaremos el modelo completo** para establecer una línea base de comparación.\n",
    "\n",
    "En full fine-tuning:\n",
    "- Entrenamos **todos** los parámetros del modelo (~67M para DistilBERT)\n",
    "- Máxima capacidad de adaptación\n",
    "- Alto uso de memoria\n",
    "- Más lento\n",
    "- Requiere más datos para evitar overfitting\n",
    "\n",
    "**Métricas a observar:**\n",
    "- Memoria GPU usada durante el entrenamiento\n",
    "- Tiempo de entrenamiento\n",
    "- Accuracy final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6b802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FULL FINE-TUNING (Entrenar todos los parámetros)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Cargar modelo base (sin LoRA)\n",
    "model_full = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "# Contar parámetros\n",
    "total_params_full = sum(p.numel() for p in model_full.parameters())\n",
    "trainable_params_full = sum(p.numel() for p in model_full.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nInformación del modelo:\")\n",
    "print(f\"Parámetros totales: {total_params_full:,}\")\n",
    "print(f\"Parámetros entrenables: {trainable_params_full:,}\")\n",
    "print(f\"Porcentaje entrenable: {100 * trainable_params_full / total_params_full:.2f}%\")\n",
    "print(f\"\\n¡Entrenamos el 100% de los parámetros!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7318b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de entrenamiento (misma que usaremos con LoRA para comparar)\n",
    "training_args_full = TrainingArguments(\n",
    "    output_dir=\"./results_full\",\n",
    "    num_train_epochs=3,              # Número de épocas\n",
    "    per_device_train_batch_size=8,   # Batch size (igual que LoRA)\n",
    "    per_device_eval_batch_size=16,   # Batch size para evaluación\n",
    "    learning_rate=2e-5,              # Learning rate (más bajo que LoRA, típico para full FT)\n",
    "    weight_decay=0.01,               # Regularización L2\n",
    "    eval_strategy=\"epoch\",           # Evaluar al final de cada época\n",
    "    save_strategy=\"no\",              # No guardar checkpoints\n",
    "    logging_steps=50,                # Log cada 50 steps\n",
    "    report_to=\"none\",                # No reportar a wandb/tensorboard\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision para ahorrar memoria\n",
    ")\n",
    "\n",
    "# Crear Trainer\n",
    "trainer_full = Trainer(\n",
    "    model=model_full,\n",
    "    args=training_args_full,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nTrainer configurado. Listo para entrenar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d422c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar y medir recursos\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Métricas de memoria\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_memory = torch.cuda.memory_allocated() / 1e9  # GB\n",
    "    print(f\"Memoria GPU inicial: {start_memory:.2f} GB\")\n",
    "\n",
    "# Medir tiempo\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"\\nIniciando entrenamiento FULL FINE-TUNING...\")\n",
    "print(\"   (Esto puede tardar ~5-10 minutos)\\n\")\n",
    "\n",
    "trainer_full.train()\n",
    "\n",
    "end_time = time.time()\n",
    "training_time_full = end_time - start_time\n",
    "\n",
    "# Reportar uso de memoria\n",
    "if torch.cuda.is_available():\n",
    "    peak_memory_full = torch.cuda.max_memory_allocated() / 1e9  # GB\n",
    "    current_memory = torch.cuda.memory_allocated() / 1e9  # GB\n",
    "    print(f\"\\nREPORTE DE MEMORIA:\")\n",
    "    print(f\"   Memoria inicial: {start_memory:.2f} GB\")\n",
    "    print(f\"   Memoria actual: {current_memory:.2f} GB\")\n",
    "    print(f\"   Memoria pico durante entrenamiento: {peak_memory_full:.2f} GB\")\n",
    "    print(f\"   Memoria adicional usada: {peak_memory_full - start_memory:.2f} GB\")\n",
    "else:\n",
    "    peak_memory_full = 0\n",
    "    print(f\"\\nEjecutando en CPU (no hay métricas de memoria GPU)\")\n",
    "\n",
    "print(f\"\\nTIEMPO DE ENTRENAMIENTO:\")\n",
    "print(f\"   {training_time_full:.2f} segundos ({training_time_full/60:.2f} minutos)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd35999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo\n",
    "print(\"\\nEvaluando modelo con Full Fine-tuning...\")\n",
    "eval_results_full = trainer_full.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTADOS - FULL FINE-TUNING\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Accuracy: {eval_results_full['eval_accuracy']:.4f}\")\n",
    "print(f\"Loss: {eval_results_full['eval_loss']:.4f}\")\n",
    "print(f\"Parámetros entrenables: {trainable_params_full:,} (100%)\")\n",
    "print(f\"Tiempo: {training_time_full:.2f}s ({training_time_full/60:.2f} min)\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Memoria pico: {peak_memory_full:.2f} GB\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Limpiar memoria para LoRA\n",
    "del model_full, trainer_full\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "print(\"\\nModelo full fine-tuning completado. Memoria liberada para LoRA.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95908cab",
   "metadata": {},
   "source": [
    "## 3. LoRA: Parameter-Efficient Fine-Tuning\n",
    "\n",
    "**Ahora veremos LoRA** - una técnica que permite entrenar modelos grandes con pocos recursos.\n",
    "En lugar de entrenar todos los parámetros, añadimos matrices pequeñas de bajo rango.\n",
    "\n",
    "### ¿Cómo funciona LoRA?\n",
    "\n",
    "En lugar de actualizar directamente las matrices de peso $W$, LoRA añade dos matrices pequeñas $A$ y $B$:\n",
    "\n",
    "$$W' = W + \\Delta W = W + BA$$\n",
    "\n",
    "Donde:\n",
    "- $W \\in \\mathbb{R}^{d \\times k}$ (matriz original, **congelada**)\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$ (matriz entrenable)\n",
    "- $A \\in \\mathbb{R}^{r \\times k}$ (matriz entrenable)\n",
    "- $r \\ll \\min(d, k)$ (rank pequeño, típicamente 4-16)\n",
    "\n",
    "### Parámetros Clave de LoRA:\n",
    "\n",
    "1. **`r` (rank)**: Dimensión del espacio de bajo rango\n",
    "   - Valores típicos: 4, 8, 16, 32\n",
    "   - Mayor rank = más parámetros entrenables = mejor capacidad pero más costoso\n",
    "   - Recomendado: empezar con 8\n",
    "\n",
    "2. **`lora_alpha`**: Factor de escalado\n",
    "   - Típicamente `2*r` o `4*r`\n",
    "   - Controla la magnitud de las actualizaciones de LoRA\n",
    "   - No afecta el número de parámetros\n",
    "\n",
    "3. **`target_modules`**: Qué capas modificar con LoRA\n",
    "   - `[\"q_lin\", \"v_lin\"]`: Solo query y value (común en transformers)\n",
    "   - `[\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\"]`: Todas las proyecciones de atención\n",
    "   - Más módulos = más parámetros entrenables\n",
    "\n",
    "4. **`lora_dropout`**: Dropout aplicado a las capas LoRA\n",
    "   - Típicamente 0.05-0.1\n",
    "   - Ayuda a prevenir overfitting\n",
    "\n",
    "**Ventajas**:\n",
    "- Solo entrenamos ~0.1-1% de parámetros\n",
    "- Mucho más rápido y eficiente en memoria\n",
    "- Ideal para múltiples tareas (puedes tener diferentes adaptadores LoRA)\n",
    "- Fácil de compartir y combinar adaptadores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ENTRENAMIENTO CON LORA (PEFT)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Cargar modelo base\n",
    "model_lora = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "# Configurar LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                                    # Rank: dimensión del espacio de bajo rango (4-32)\n",
    "    lora_alpha=32,                          # Scaling factor: típicamente 2*r o 4*r\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],      # Módulos a adaptar: query y value attention\n",
    "    lora_dropout=0.1,                       # Dropout para regularización\n",
    "    bias=\"none\",                            # No entrenar bias adicionales\n",
    "    task_type=TaskType.SEQ_CLS              # Tipo de tarea: Sequence Classification\n",
    ")\n",
    "\n",
    "# Aplicar LoRA al modelo\n",
    "model_lora = get_peft_model(model_lora, lora_config)\n",
    "\n",
    "# Imprimir información del modelo\n",
    "print(\"\\nInformación del modelo con LoRA:\")\n",
    "model_lora.print_trainable_parameters()\n",
    "\n",
    "# Contar parámetros manualmente para comparación\n",
    "total_params_lora = sum(p.numel() for p in model_lora.parameters())\n",
    "trainable_params_lora = sum(p.numel() for p in model_lora.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nRESUMEN:\")\n",
    "print(f\"Parámetros totales: {total_params_lora:,}\")\n",
    "print(f\"Parámetros entrenables: {trainable_params_lora:,}\")\n",
    "print(f\"Porcentaje entrenable: {100 * trainable_params_lora / total_params_lora:.2f}%\")\n",
    "print(f\"\\n¡Solo entrenamos ~1% de los parámetros del modelo!\")\n",
    "print(f\"\\nCon rank r={lora_config.r}:\")\n",
    "print(f\"  - Cada matriz LoRA añade: d×r + r×k parámetros\")\n",
    "print(f\"  - Para attention de dim 768: ~{(768*8 + 8*768)*2:,} params por capa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f74fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de entrenamiento\n",
    "training_args_lora = TrainingArguments(\n",
    "    output_dir=\"./results_lora\",\n",
    "    num_train_epochs=3,              # Número de épocas completas de entrenamiento\n",
    "    per_device_train_batch_size=8,   # Batch size para entrenamiento\n",
    "    per_device_eval_batch_size=16,   # Batch size para evaluación (puede ser mayor)\n",
    "    learning_rate=2e-4,              # Learning rate (típicamente más alto con LoRA: 1e-4 a 3e-4)\n",
    "    weight_decay=0.01,               # Regularización L2\n",
    "    eval_strategy=\"epoch\",           # Evaluar al final de cada época\n",
    "    save_strategy=\"no\",              # No guardar checkpoints (para rapidez)\n",
    "    logging_steps=50,                # Log cada 50 steps\n",
    "    report_to=\"none\",                # No reportar a wandb/tensorboard\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision training si hay GPU\n",
    ")\n",
    "\n",
    "# Crear Trainer\n",
    "trainer_lora = Trainer(\n",
    "    model=model_lora,\n",
    "    args=training_args_lora,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678349a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar y medir tiempo/memoria\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Métricas de memoria\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_memory_lora = torch.cuda.memory_allocated() / 1e9  # GB\n",
    "    print(f\"Memoria GPU inicial: {start_memory_lora:.2f} GB\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"\\nIniciando entrenamiento con LoRA...\")\n",
    "print(\"   (Esto debería ser más rápido que full fine-tuning)\\n\")\n",
    "\n",
    "trainer_lora.train()\n",
    "\n",
    "end_time = time.time()\n",
    "training_time_lora = end_time - start_time\n",
    "\n",
    "# Reportar uso de memoria\n",
    "if torch.cuda.is_available():\n",
    "    peak_memory_lora = torch.cuda.max_memory_allocated() / 1e9  # GB\n",
    "    current_memory_lora = torch.cuda.memory_allocated() / 1e9  # GB\n",
    "    print(f\"\\nREPORTE DE MEMORIA:\")\n",
    "    print(f\"   Memoria inicial: {start_memory_lora:.2f} GB\")\n",
    "    print(f\"   Memoria actual: {current_memory_lora:.2f} GB\")\n",
    "    print(f\"   Memoria pico durante entrenamiento: {peak_memory_lora:.2f} GB\")\n",
    "    print(f\"   Memoria adicional usada: {peak_memory_lora - start_memory_lora:.2f} GB\")\n",
    "else:\n",
    "    peak_memory_lora = 0\n",
    "    print(f\"\\nEjecutando en CPU (no hay métricas de memoria GPU)\")\n",
    "\n",
    "print(f\"\\nTIEMPO DE ENTRENAMIENTO:\")\n",
    "print(f\"   {training_time_lora:.2f} segundos ({training_time_lora/60:.2f} minutos)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe169aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar\n",
    "print(\"\\nEvaluando modelo con LoRA...\")\n",
    "eval_results_lora = trainer_lora.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTADOS - LORA (PEFT)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Accuracy: {eval_results_lora['eval_accuracy']:.4f}\")\n",
    "print(f\"Loss: {eval_results_lora['eval_loss']:.4f}\")\n",
    "print(f\"Parámetros entrenables: {trainable_params_lora:,} ({100 * trainable_params_lora / total_params_lora:.2f}%)\")\n",
    "print(f\"Tiempo: {training_time_lora:.2f}s ({training_time_lora/60:.2f} min)\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Memoria pico: {peak_memory_lora:.2f} GB\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nLoRA permite hacer fine-tuning de modelos grandes eficientemente\")\n",
    "print(\"   Ideal para: recursos limitados, múltiples tareas, experimentación rápida\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2819f40a",
   "metadata": {},
   "source": [
    "### 3.1 Comparación Directa: Full Fine-tuning vs LoRA\n",
    "\n",
    "Comparemos los resultados lado a lado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ea15d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Crear tabla comparativa\n",
    "comparison_data = {\n",
    "    'Métrica': [\n",
    "        'Accuracy',\n",
    "        'Loss',\n",
    "        'Parámetros Entrenables',\n",
    "        '% Parámetros',\n",
    "        'Tiempo (segundos)',\n",
    "        'Tiempo (minutos)',\n",
    "        'Memoria Pico (GB)',\n",
    "        'Speedup vs Full FT',\n",
    "        'Ahorro Memoria vs Full FT'\n",
    "    ],\n",
    "    'Full Fine-tuning': [\n",
    "        f\"{eval_results_full['eval_accuracy']:.4f}\",\n",
    "        f\"{eval_results_full['eval_loss']:.4f}\",\n",
    "        f\"{trainable_params_full:,}\",\n",
    "        \"100.00%\",\n",
    "        f\"{training_time_full:.2f}\",\n",
    "        f\"{training_time_full/60:.2f}\",\n",
    "        f\"{peak_memory_full:.2f}\" if torch.cuda.is_available() else \"N/A (CPU)\",\n",
    "        \"1.00x (baseline)\",\n",
    "        \"0% (baseline)\"\n",
    "    ],\n",
    "    'LoRA': [\n",
    "        f\"{eval_results_lora['eval_accuracy']:.4f}\",\n",
    "        f\"{eval_results_lora['eval_loss']:.4f}\",\n",
    "        f\"{trainable_params_lora:,}\",\n",
    "        f\"{100 * trainable_params_lora / total_params_lora:.2f}%\",\n",
    "        f\"{training_time_lora:.2f}\",\n",
    "        f\"{training_time_lora/60:.2f}\",\n",
    "        f\"{peak_memory_lora:.2f}\" if torch.cuda.is_available() else \"N/A (CPU)\",\n",
    "        f\"{training_time_full/training_time_lora:.2f}x\" if training_time_lora > 0 else \"N/A\",\n",
    "        f\"{100*(peak_memory_full - peak_memory_lora)/peak_memory_full:.1f}%\" if torch.cuda.is_available() and peak_memory_full > 0 else \"N/A\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARACIÓN: FULL FINE-TUNING vs LoRA\")\n",
    "print(\"=\" * 80)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Análisis de resultados\n",
    "print(\"\\nANÁLISIS:\")\n",
    "print(f\"\\n1. PRECISIÓN:\")\n",
    "acc_diff = eval_results_lora['eval_accuracy'] - eval_results_full['eval_accuracy']\n",
    "if abs(acc_diff) < 0.01:\n",
    "    print(f\"   Accuracy similar entre ambos métodos (diferencia: {acc_diff:+.4f})\")\n",
    "else:\n",
    "    print(f\"   Accuracy diferente: {acc_diff:+.4f}\")\n",
    "\n",
    "print(f\"\\n2. EFICIENCIA DE PARÁMETROS:\")\n",
    "param_reduction = 100 * (1 - trainable_params_lora / trainable_params_full)\n",
    "print(f\"   LoRA entrena {param_reduction:.1f}% menos parámetros\")\n",
    "print(f\"   Solo {trainable_params_lora:,} parámetros vs {trainable_params_full:,}\")\n",
    "\n",
    "print(f\"\\n3. VELOCIDAD:\")\n",
    "if training_time_lora > 0:\n",
    "    speedup = training_time_full / training_time_lora\n",
    "    time_saved = training_time_full - training_time_lora\n",
    "    print(f\"   LoRA es {speedup:.2f}x más rápido\")\n",
    "    print(f\"   Ahorro de tiempo: {time_saved:.1f} segundos ({time_saved/60:.1f} minutos)\")\n",
    "\n",
    "if torch.cuda.is_available() and peak_memory_full > 0:\n",
    "    print(f\"\\n4. MEMORIA:\")\n",
    "    memory_saved = peak_memory_full - peak_memory_lora\n",
    "    memory_reduction = 100 * memory_saved / peak_memory_full\n",
    "    print(f\"   LoRA usa {memory_reduction:.1f}% menos memoria\")\n",
    "    print(f\"   Ahorro: {memory_saved:.2f} GB\")\n",
    "    print(f\"   Esto permite entrenar modelos más grandes en la misma GPU\")\n",
    "\n",
    "print(f\"\\nCONCLUSIÓN:\")\n",
    "print(f\"   LoRA ofrece un excelente balance entre eficiencia y rendimiento.\")\n",
    "print(f\"   Es ideal cuando tienes recursos limitados o necesitas entrenar rápido.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842bcf9",
   "metadata": {},
   "source": [
    "## 4. Experimentando con LoRA\n",
    "\n",
    "### 4.1 Efecto del Rank (r)\n",
    "\n",
    "El parámetro más importante en LoRA es el **rank** ($r$). Veamos cómo afecta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aab658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar diferentes ranks\n",
    "ranks = [4, 8, 16]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARACIÓN DE RANKS EN LORA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for r in ranks:\n",
    "    # Crear modelo con LoRA\n",
    "    model_temp = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    lora_config_temp = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_lin\", \"v_lin\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS\n",
    "    )\n",
    "    \n",
    "    model_temp = get_peft_model(model_temp, lora_config_temp)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model_temp.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model_temp.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nRank r={r}:\")\n",
    "    print(f\"  Parámetros entrenables: {trainable_params:,}\")\n",
    "    print(f\"  Porcentaje: {100 * trainable_params / total_params:.2f}%\")\n",
    "    \n",
    "    del model_temp\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nObservaciones:\")\n",
    "print(\"  - Ranks más altos = más parámetros entrenables\")\n",
    "print(\"  - Pero incluso con r=16, seguimos en ~2% de parámetros\")\n",
    "print(\"  - En la práctica: r=4-8 suele ser suficiente\")\n",
    "print(\"  - r muy alto puede causar overfitting en datasets pequeños\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67b229",
   "metadata": {},
   "source": [
    "## 5. Ejercicios Propuestos\n",
    "\n",
    "### Ejercicio 1: Experimentar con Diferentes Ranks\n",
    "Entrena modelos con r=4, r=8 y r=16. Compara:\n",
    "- Accuracy final\n",
    "- Tiempo de entrenamiento\n",
    "- Memoria usada\n",
    "\n",
    "### Ejercicio 2: Más Módulos Target\n",
    "Prueba aplicar LoRA a más capas:\n",
    "```python\n",
    "target_modules=[\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\"]\n",
    "```\n",
    "¿Cómo afecta al rendimiento?\n",
    "\n",
    "### Ejercicio 3: Entrenar con Dataset Completo\n",
    "Entrena con los 25K ejemplos completos del dataset IMDB:\n",
    "- ¿Mejora el accuracy?\n",
    "- ¿Cuánto tiempo toma?\n",
    "- ¿Sigue siendo eficiente LoRA?\n",
    "\n",
    "### Ejercicio 4: Otros Modelos\n",
    "Prueba LoRA con otros modelos:\n",
    "- `bert-base-uncased`\n",
    "- `roberta-base`\n",
    "- Modelos más grandes si tienes GPU\n",
    "\n",
    "### Ejercicio 5: Datasets Diferentes\n",
    "Aplica LoRA a otros tasks:\n",
    "- `sst2` (sentimiento)\n",
    "- `mrpc` (paráfrasis)\n",
    "- `cola` (aceptabilidad gramatical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb63deb",
   "metadata": {},
   "source": [
    "## 6. Resumen\n",
    "\n",
    "### ¿Qué aprendimos?\n",
    "\n",
    "En este notebook comparamos **Full Fine-tuning** vs **LoRA (Low-Rank Adaptation)**:\n",
    "\n",
    "**Full Fine-tuning**:\n",
    "- Máxima capacidad de adaptación\n",
    "- Alto uso de memoria (varios GB)\n",
    "- Más lento\n",
    "- Requiere más recursos\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)**:\n",
    "- Entrena solo ~1% de parámetros\n",
    "- **Eficiencia**: Entrenar solo ~1% de los parámetros del modelo\n",
    "- **Velocidad**: 3-5x más rápido que full fine-tuning\n",
    "- **Memoria**: Usa ~50-70% menos memoria\n",
    "- **Modularidad**: Múltiples adaptadores para diferentes tareas\n",
    "\n",
    "### Parámetros Críticos para Recordar:\n",
    "\n",
    "| Parámetro | Valores Típicos | Efecto |\n",
    "|-----------|----------------|--------|\n",
    "| **r** (rank) | 4, 8, 16 | Mayor = más capacidad, más parámetros |\n",
    "| **lora_alpha** | 16, 32, 64 | Típicamente 2*r o 4*r |\n",
    "| **target_modules** | [\"q_lin\", \"v_lin\"] | Qué capas adaptar |\n",
    "| **learning_rate** | 1e-4 a 3e-4 | Mayor que full fine-tuning |\n",
    "\n",
    "### Cuándo usar LoRA:\n",
    "\n",
    "**SÍ usar LoRA cuando:**\n",
    "- Recursos limitados (GPU pequeña o CPU)\n",
    "- Necesitas entrenar rápidamente\n",
    "- Múltiples tareas/dominios con el mismo modelo base\n",
    "- Experimentación rápida con diferentes configuraciones\n",
    "\n",
    "**NO usar LoRA cuando:**\n",
    "- Tienes recursos ilimitados y necesitas máximo rendimiento\n",
    "- La tarea es muy diferente del pretraining\n",
    "- Dataset extremadamente grande y específico\n",
    "\n",
    "### Próximos Pasos:\n",
    "\n",
    "1. **Experimenta** con diferentes ranks (r=4, 8, 16, 32)\n",
    "2. **Prueba** más target_modules\n",
    "3. **Compara** con full fine-tuning en tu tarea específica\n",
    "4. **Explora** QLoRA para modelos aún más grandes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d3cbf6",
   "metadata": {},
   "source": [
    "## 7. Recursos Adicionales\n",
    "\n",
    "### Papers Importantes:\n",
    "- **LoRA**: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
    "- **QLoRA**: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
    "- **Prefix Tuning**: [Prefix-Tuning: Optimizing Continuous Prompts](https://arxiv.org/abs/2101.00190)\n",
    "\n",
    "### Librerías:\n",
    "- **Hugging Face PEFT**: https://github.com/huggingface/peft\n",
    "- **Documentación PEFT**: https://huggingface.co/docs/peft\n",
    "\n",
    "### Investiga otras técnicas PEFT:\n",
    "- **Adapter Layers**: Añadir capas pequeñas entre layers\n",
    "- **Prompt Tuning**: Solo entrenar los embeddings del prompt\n",
    "- **QLoRA**: LoRA + 4-bit quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47215ee",
   "metadata": {},
   "source": [
    "Creado por Jorge Dueñas Lerín\n",
    "jorge.duenas.lerin@upm.es"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
