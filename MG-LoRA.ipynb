{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üíß LFM2 - SFT with TRL\n",
        "\n",
        "This tutorial demonstrates how to fine-tune our LFM2 models, e.g. [`LiquidAI/LFM2-1.2B`](https://huggingface.co/LiquidAI/LFM2-1.2B), using the TRL library.\n",
        "\n",
        "Follow along if it's your first time using trl, or take single code snippets for your own workflow\n",
        "\n",
        "## üéØ What you'll find:\n",
        "- **SFT** (Supervised Fine-Tuning) - Basic instruction following\n",
        "- **LoRA + SFT** - Using LoRA (from PEFT) to SFT while on constrained hardware\n",
        "\n",
        "## üìã Prerequisites:\n",
        "- **GPU Runtime**: Select GPU in `Runtime` ‚Üí `Change runtime type`\n",
        "- **Hugging Face Account**: For accessing models and datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üì¶ Installation & Setup\n",
        "\n",
        "First, let's install all the required packages:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
            "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 73ms\u001b[0m\u001b[0m\r\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m\r\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%uv pip install transformers==4.54.0 trl>=0.18.2 peft>=0.15.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now verify the packages are installed correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ PyTorch version: 2.8.0+cu129\n",
            "ü§ó Transformers version: 4.54.0\n",
            "üìä TRL version: 0.20.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import trl\n",
        "import os\n",
        "\n",
        "print(f\"üì¶ PyTorch version: {torch.__version__}\")\n",
        "print(f\"ü§ó Transformers version: {transformers.__version__}\")\n",
        "print(f\"üìä TRL version: {trl.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading the model from Transformers ü§ó\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìö Loading tokenizer...\n",
            "üß† Loading model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9ea6d29a9e54bc8898dff32ddc750bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Local model loaded successfully!\n",
            "üî¢ Parameters: 4,300,079,472\n",
            "üìñ Vocab size: 262145\n",
            "üíæ Model size: ~8.6 GB (bfloat16)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "model_id = \"google/gemma-3-4b-it\"\n",
        "print(\"üìö Loading tokenizer...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "print(\"üß† Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"bfloat16\",\n",
        "#   attn_implementation=\"flash_attention_2\" #<- uncomment on compatible GPU\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Local model loaded successfully!\")\n",
        "print(f\"üî¢ Parameters: {model.num_parameters():,}\")\n",
        "print(f\"üìñ Vocab size: {len(tokenizer)}\")\n",
        "print(f\"üíæ Model size: ~{model.num_parameters() * 2 / 1e9:.1f} GB (bfloat16)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Part 1: Supervised Fine-Tuning (SFT)\n",
        "\n",
        "SFT teaches the model to follow instructions by training on input-output pairs (instruction vs response). This is the foundation for creating instruction-following models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load an SFT Dataset\n",
        "\n",
        "We will use [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk), limiting ourselves to the first 5k samples for brevity. Feel free to change the limit by changing the slicing index in the parameter `split`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Loading SFT dataset...\n",
            "‚úÖ SFT Dataset loaded:\n",
            "   üìö Train samples: 7950\n",
            "   üß™ Eval samples: 884\n",
            "\n",
            "üìù Single Sample: You are a patient that has gone to do an interview with a psychologist. The psychologist will ask you a series of questions and you will answer them in a natural way:\n",
            "\n",
            "### Input:\n",
            "mhm .\n",
            "\n",
            "### Expected Response:\n",
            "and they have like a dredge .\n",
            " it's a big suction hose .\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(\"üì• Loading SFT dataset...\")\n",
        "dataset = load_dataset('json', data_files='./discourse_qa.json')\n",
        "dataset = dataset.remove_columns([\"question\", \"answer\"])\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "dataset = dataset[\"train\"].train_test_split(0.1,seed=42)\n",
        "train_dataset_sft = dataset[\"train\"]\n",
        "eval = dataset['test'].train_test_split(0.5, seed=42)\n",
        "eval_dataset_sft = dataset[\"test\"]\n",
        "\n",
        "print(\"‚úÖ SFT Dataset loaded:\")\n",
        "print(f\"   üìö Train samples: {len(train_dataset_sft)}\")\n",
        "print(f\"   üß™ Eval samples: {len(eval_dataset_sft)}\")\n",
        "print(f\"\\nüìù Single Sample: {train_dataset_sft[0]['text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéõÔ∏è Part 2: LoRA + SFT (Parameter-Efficient Fine-tuning)\n",
        "\n",
        "LoRA (Low-Rank Adaptation) allows efficient fine-tuning by only training a small number of additional parameters. Perfect for limited compute resources!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wrap the model with PEFT\n",
        "\n",
        "We specify target modules that will be finetuned while the rest of the models weights remains frozen. Feel free to modify the `r` (rank) value:\n",
        "- higher -> better approximation of full-finetuning\n",
        "- lower -> needs even less compute resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 41,549,824 || all params: 4,341,629,296 || trainable%: 0.9570\n",
            "‚úÖ LoRA configuration applied!\n",
            "üéõÔ∏è  LoRA rank: 64\n",
            "üìä LoRA alpha: 16\n",
            "üéØ Target modules: {'w1', 'v_proj', 'k_proj', 'in_proj', 'w3', 'w2', 'out_proj', 'q_proj'}\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "GLU_MODULES = [\"w1\", \"w2\", \"w3\"]\n",
        "MHA_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
        "CONV_MODULES = [\"in_proj\", \"out_proj\"]\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=16,  # <- lower values = fewer parameters\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=GLU_MODULES + MHA_MODULES + CONV_MODULES,\n",
        "    bias=\"none\",\n",
        "    modules_to_save=None,\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model, lora_config)\n",
        "lora_model.print_trainable_parameters()\n",
        "\n",
        "print(\"‚úÖ LoRA configuration applied!\")\n",
        "print(f\"üéõÔ∏è  LoRA rank: {lora_config.r}\")\n",
        "print(f\"üìä LoRA alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"üéØ Target modules: {lora_config.target_modules}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch Training\n",
        "\n",
        "Now ready to launch the SFT training, but this time with the LoRA-wrapped model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèóÔ∏è  Creating LoRA SFT trainer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ Starting LoRA + SFT training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1199' max='39750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 1199/39750 06:43 < 3:36:33, 2.97 it/s, Epoch 0.30/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "lora_sft_config = SFTConfig(\n",
        "    output_dir=\"./lfm2-sft-lora\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=2,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_steps=100,\n",
        "    warmup_ratio=0.2,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=None,\n",
        ")\n",
        "\n",
        "print(\"üèóÔ∏è  Creating LoRA SFT trainer...\")\n",
        "lora_sft_trainer = SFTTrainer(\n",
        "    model=lora_model,\n",
        "    args=lora_sft_config,\n",
        "    train_dataset=train_dataset_sft,\n",
        "    eval_dataset=eval_dataset_sft,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Starting LoRA + SFT training...\")\n",
        "lora_sft_trainer.train()\n",
        "\n",
        "print(\"üéâ LoRA + SFT training completed!\")\n",
        "\n",
        "lora_sft_trainer.save_model()\n",
        "print(f\"üíæ LoRA model saved to: {lora_sft_config.output_dir}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
